{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Plan Link to these pages: We have a Kubernetes cluster ... in which there is an example java application Alternatives for building docker image Metrics and graphs K6 soak testing What to do when it is not running correctly Connecting to application in debug mode Java Flight Recorder Java Mission Control Other tools and tricks of the trade What have we learned? What to learn More options to debug applications in Kubernetes Soak testing Metrics help you but determine baseline beforehand Plan ahead: How are you going to debug an application? Give and revoke temporary access Method for container introspection Document how to attach a container","title":"0. Plan"},{"location":"#plan","text":"Link to these pages: We have a Kubernetes cluster ... in which there is an example java application Alternatives for building docker image Metrics and graphs K6 soak testing What to do when it is not running correctly Connecting to application in debug mode Java Flight Recorder Java Mission Control Other tools and tricks of the trade What have we learned?","title":"Plan"},{"location":"#what-to-learn","text":"More options to debug applications in Kubernetes Soak testing Metrics help you but determine baseline beforehand Plan ahead: How are you going to debug an application? Give and revoke temporary access Method for container introspection Document how to attach a container","title":"What to learn"},{"location":"01-all-set-up/","text":"Our setup We have now prepared a Kubernetes cluster with a running java application. We have added a set of tools to help us diagnose trouble. In your real life project, you most likely will have to add these tools after-the-fact, i.e. when you find you need to have a harder look at a problem. Our resources and setup: Nginx entrypoint: http://localhost:31090/ Our application ##5: http://k8sdebug.local.gd:31090/ Metrics endpoint for our application http://k8sdebug.local.gd:31090/actuator/prometheus Prometheus interface: Prometheus with memory used query Grafana: http://grafana.local.gd:31090/dashboards K6 dashboard in grafana: http://grafana.local.gd:31090/d/ccbb2351-2ae2-462f-ae0e-f2c893ad1028/k6-prometheus?orgId=1 Dashboard in Grafana Opening http://grafana.local.gd:31090/d/vJAZ9jwWk/jvm-namespace-centric?orgId=1&refresh=5s show you - among other things - that the heap is quite full . This is due to JIB magic. You can adjust some setting by environment variables, which requires you to understand and this specific product. Understanding the exact needs of an application, and where and how the memory is actually used, is taxing, and usually memory get blindly adjusted upwards until the application works.","title":"1. Our setup"},{"location":"01-all-set-up/#our-setup","text":"We have now prepared a Kubernetes cluster with a running java application. We have added a set of tools to help us diagnose trouble. In your real life project, you most likely will have to add these tools after-the-fact, i.e. when you find you need to have a harder look at a problem. Our resources and setup: Nginx entrypoint: http://localhost:31090/ Our application ##5: http://k8sdebug.local.gd:31090/ Metrics endpoint for our application http://k8sdebug.local.gd:31090/actuator/prometheus Prometheus interface: Prometheus with memory used query Grafana: http://grafana.local.gd:31090/dashboards K6 dashboard in grafana: http://grafana.local.gd:31090/d/ccbb2351-2ae2-462f-ae0e-f2c893ad1028/k6-prometheus?orgId=1","title":"Our setup"},{"location":"01-all-set-up/#dashboard-in-grafana","text":"Opening http://grafana.local.gd:31090/d/vJAZ9jwWk/jvm-namespace-centric?orgId=1&refresh=5s show you - among other things - that the heap is quite full . This is due to JIB magic. You can adjust some setting by environment variables, which requires you to understand and this specific product. Understanding the exact needs of an application, and where and how the memory is actually used, is taxing, and usually memory get blindly adjusted upwards until the application works.","title":"Dashboard in Grafana"},{"location":"02-kind/","text":"Kind installation for Kubernetes For installation of base tools and other setup details, see: https://nostra.github.io/neon/#/1/1 https://nostra.github.io/gitops-fluxcd2/ Using Colima on Mac We want to increase the number of file handles: colima ssh Edit /etc/sysctl.conf and add: sudo -i cat <<EOF >> /etc/sysctl.conf fs.inotify.max_user_watches = 1048576 fs.inotify.max_user_instances = 512 EOF Then systctl -p Base setup / setup of your local kubernetes Create cluster: kind create cluster --config kind-api-cluster.yaml --name=k8sdebug Test cluster: (##1) kubectl cluster-info --context kind-k8sdebug kubectl wait pod \\ --all \\ --for=condition=Ready -A Create a key that you will register as a deploy key in github : ssh-keygen -f ~/.ssh/k8sdebug -N \"\" -C \"Key for k8sdebug setup. This key was generated by $USER\" Copy the key and create it as a deploy key with write rights in https://github.com/nostra/k8sdebug/settings/keys cat $HOME/.ssh/k8sdebug.pub | pbcopy Bootstrapping Flux Bootstrapping flux: cd .. kubectl create -k flux/system kubectl wait pod \\ --all \\ --for=condition=Ready -n flux-system --timeout 300s Create the local cluster: cd .. kubectl create -k flux/kind/ Create secret in the flux system The secret gets created so you can pull (and later write) to the git repository: flux create secret git flux-cluster \\ --url=ssh://git@github.com/nostra/k8sdebug \\ --private-key-file=$HOME/.ssh/k8sdebug --namespace=flux-system \\ --export | kubectl create -f - Create a pull token to fetch images from github packages First just check that you have the namespace \"apps\": kubectl get ns apps If you don't have it, check running pods, and status of flux. (See debug section at the bottom of this file for details.) Ref: authenticating-with-a-personal-access-token-classic open: https://github.com/settings/tokens Choose \"Generate new token\" and classic token Fill in info Choose write:packages and delete:packages (for good measure) Click \"Generate token\" write something like export CR_PAT=\"...\" export USERNAME=\"your github handle\" echo $CR_PAT | docker login ghcr.io -u $USERNAME --password-stdin docker pull ghcr.io/k8sdebug/k8sdebug-app:0.0.12 If you with this manage to pull an image, you successfully have a token in CR_PAT variable. export CR_PAT=\"...\" export USERNAME=\"your github handle\" kubectl -n apps \\ create secret docker-registry gh-packages-auth --docker-server=ghcr.io --docker-username=$USERNAME \\ --docker-password=\"$CR_PAT\" --dry-run=client -o yaml > k8sdebug-pull-secret.yaml kubectl create -f k8sdebug-pull-secret.yaml rm k8sdebug-pull-secret.yaml (Take a moment ot look at the yaml file before applying and deleting it, in order to verify contents.) Optional: Enable mcalert In order to get a nifty display of cluster status on the menubar, install https://github.com/nostra/mcalert The github entry page will explain how you compile and install it. Create the file .mcalert.properties in your $HOME and put the following into it: mcalert.prometheus.endpoints.local-prometheus.ignore-alerts=CPUThrottlingHigh,KubeControllerManagerDown,KubeControllerManagerDown,KubeSchedulerDown,NodeClockNotSynchronising mcalert.prometheus.endpoints.local-prometheus.watchdog-alerts=disabled mcalert.prometheus.endpoints.local-prometheus.uri=http://prometheus.local.gd:31090/api/v1/alerts mcalert.prometheus.endpoints.local-prometheus-auth.ignore-alerts=CPUThrottlingHigh,KubeControllerManagerDown,KubeControllerManagerDown,KubeSchedulerDown,NodeClockNotSynchronising mcalert.prometheus.endpoints.local-prometheus-auth.watchdog-alerts=disabled mcalert.prometheus.endpoints.local-prometheus-auth.uri=http://prometheus-auth.local.gd:31090/api/v1/alerts mcalert.prometheus.endpoints.local-prometheus-auth.header[0].name=Authorization mcalert.prometheus.endpoints.local-prometheus-auth.header[0].content=Basic bmVvbjpzZXNhbQ== Commands / cheat sheet Current alerts: Command Description kubectl get pods -A Get pods in all namespaces. All should be running ok flux get kustomization -A Get Kustomizations for all namespaces. They should all not be suspended, flux get sources git Find status of sources, i.e. the remote git repostory flux reconcile source git k8sdebug Force reconcilliation of git flux suspend kustomization -n flux-system flux-cluster Stop reconcillation. Start it again with \"resume\" kubectl config set-context --current --namespace=apps Set active namespace to apps If trouble with nginx-proxy and prometheus kubectl port-forward -n monitoring svc/prometheus-k8s 9090:9090 Then: http://localhost:9090/alerts","title":"2. Kubernetes"},{"location":"02-kind/#kind-installation-for-kubernetes","text":"For installation of base tools and other setup details, see: https://nostra.github.io/neon/#/1/1 https://nostra.github.io/gitops-fluxcd2/","title":"Kind installation for Kubernetes"},{"location":"02-kind/#using-colima-on-mac","text":"We want to increase the number of file handles: colima ssh Edit /etc/sysctl.conf and add: sudo -i cat <<EOF >> /etc/sysctl.conf fs.inotify.max_user_watches = 1048576 fs.inotify.max_user_instances = 512 EOF Then systctl -p","title":"Using Colima on Mac"},{"location":"02-kind/#base-setup-setup-of-your-local-kubernetes","text":"Create cluster: kind create cluster --config kind-api-cluster.yaml --name=k8sdebug Test cluster: (##1) kubectl cluster-info --context kind-k8sdebug kubectl wait pod \\ --all \\ --for=condition=Ready -A Create a key that you will register as a deploy key in github : ssh-keygen -f ~/.ssh/k8sdebug -N \"\" -C \"Key for k8sdebug setup. This key was generated by $USER\" Copy the key and create it as a deploy key with write rights in https://github.com/nostra/k8sdebug/settings/keys cat $HOME/.ssh/k8sdebug.pub | pbcopy","title":"Base setup / setup of your local kubernetes"},{"location":"02-kind/#bootstrapping-flux","text":"Bootstrapping flux: cd .. kubectl create -k flux/system kubectl wait pod \\ --all \\ --for=condition=Ready -n flux-system --timeout 300s Create the local cluster: cd .. kubectl create -k flux/kind/","title":"Bootstrapping Flux"},{"location":"02-kind/#create-secret-in-the-flux-system","text":"The secret gets created so you can pull (and later write) to the git repository: flux create secret git flux-cluster \\ --url=ssh://git@github.com/nostra/k8sdebug \\ --private-key-file=$HOME/.ssh/k8sdebug --namespace=flux-system \\ --export | kubectl create -f -","title":"Create secret in the flux system"},{"location":"02-kind/#create-a-pull-token-to-fetch-images-from-github-packages","text":"First just check that you have the namespace \"apps\": kubectl get ns apps If you don't have it, check running pods, and status of flux. (See debug section at the bottom of this file for details.) Ref: authenticating-with-a-personal-access-token-classic open: https://github.com/settings/tokens Choose \"Generate new token\" and classic token Fill in info Choose write:packages and delete:packages (for good measure) Click \"Generate token\" write something like export CR_PAT=\"...\" export USERNAME=\"your github handle\" echo $CR_PAT | docker login ghcr.io -u $USERNAME --password-stdin docker pull ghcr.io/k8sdebug/k8sdebug-app:0.0.12 If you with this manage to pull an image, you successfully have a token in CR_PAT variable. export CR_PAT=\"...\" export USERNAME=\"your github handle\" kubectl -n apps \\ create secret docker-registry gh-packages-auth --docker-server=ghcr.io --docker-username=$USERNAME \\ --docker-password=\"$CR_PAT\" --dry-run=client -o yaml > k8sdebug-pull-secret.yaml kubectl create -f k8sdebug-pull-secret.yaml rm k8sdebug-pull-secret.yaml (Take a moment ot look at the yaml file before applying and deleting it, in order to verify contents.)","title":"Create a pull token to fetch images from github packages"},{"location":"02-kind/#optional-enable-mcalert","text":"In order to get a nifty display of cluster status on the menubar, install https://github.com/nostra/mcalert The github entry page will explain how you compile and install it. Create the file .mcalert.properties in your $HOME and put the following into it: mcalert.prometheus.endpoints.local-prometheus.ignore-alerts=CPUThrottlingHigh,KubeControllerManagerDown,KubeControllerManagerDown,KubeSchedulerDown,NodeClockNotSynchronising mcalert.prometheus.endpoints.local-prometheus.watchdog-alerts=disabled mcalert.prometheus.endpoints.local-prometheus.uri=http://prometheus.local.gd:31090/api/v1/alerts mcalert.prometheus.endpoints.local-prometheus-auth.ignore-alerts=CPUThrottlingHigh,KubeControllerManagerDown,KubeControllerManagerDown,KubeSchedulerDown,NodeClockNotSynchronising mcalert.prometheus.endpoints.local-prometheus-auth.watchdog-alerts=disabled mcalert.prometheus.endpoints.local-prometheus-auth.uri=http://prometheus-auth.local.gd:31090/api/v1/alerts mcalert.prometheus.endpoints.local-prometheus-auth.header[0].name=Authorization mcalert.prometheus.endpoints.local-prometheus-auth.header[0].content=Basic bmVvbjpzZXNhbQ==","title":"Optional: Enable mcalert"},{"location":"02-kind/#commands-cheat-sheet","text":"Current alerts: Command Description kubectl get pods -A Get pods in all namespaces. All should be running ok flux get kustomization -A Get Kustomizations for all namespaces. They should all not be suspended, flux get sources git Find status of sources, i.e. the remote git repostory flux reconcile source git k8sdebug Force reconcilliation of git flux suspend kustomization -n flux-system flux-cluster Stop reconcillation. Start it again with \"resume\" kubectl config set-context --current --namespace=apps Set active namespace to apps","title":"Commands / cheat sheet"},{"location":"02-kind/#if-trouble-with-nginx-proxy-and-prometheus","text":"kubectl port-forward -n monitoring svc/prometheus-k8s 9090:9090 Then: http://localhost:9090/alerts","title":"If trouble with nginx-proxy and prometheus"},{"location":"03-java-application/","text":"Java example spring boot application A simple spring-boot application created with Spring Initializr. Micrometer metrics are enabled and exposed with Prometheus (##2) We have a custom metric Build / deploy Build: cd ../spring-app ./mvnw package spring-boot:build-image This builds and sets up the image (##3) with JiB. The setup takes a considerable amount of time... You get an image primed with a rather obtuse setup with values you might want to adjust. https://github.com/GoogleContainerTools/jib?tab=readme-ov-file#jib Alternative jib build Just build the image yourself. Instructions: https://docs.spring.io/spring-boot/reference/packaging/container-images/dockerfiles.html cd ../spring-app ./mvnw package docker build -t k8sdebug:manual . Deploy image to kind kind load docker-image k8sdebug:0.0.1-SNAPSHOT --name k8sdebug kind load docker-image k8sdebug:manual --name k8sdebug","title":"3. A java application"},{"location":"03-java-application/#java-example-spring-boot-application","text":"A simple spring-boot application created with Spring Initializr. Micrometer metrics are enabled and exposed with Prometheus (##2) We have a custom metric","title":"Java example spring boot application"},{"location":"03-java-application/#build-deploy","text":"Build: cd ../spring-app ./mvnw package spring-boot:build-image This builds and sets up the image (##3) with JiB. The setup takes a considerable amount of time... You get an image primed with a rather obtuse setup with values you might want to adjust. https://github.com/GoogleContainerTools/jib?tab=readme-ov-file#jib","title":"Build / deploy"},{"location":"03-java-application/#alternative-jib-build","text":"Just build the image yourself. Instructions: https://docs.spring.io/spring-boot/reference/packaging/container-images/dockerfiles.html cd ../spring-app ./mvnw package docker build -t k8sdebug:manual .","title":"Alternative jib build"},{"location":"03-java-application/#deploy-image-to-kind","text":"kind load docker-image k8sdebug:0.0.1-SNAPSHOT --name k8sdebug kind load docker-image k8sdebug:manual --name k8sdebug","title":"Deploy image to kind"},{"location":"04-k6s/","text":"K6 K6 could have been named Kapablo . https://grafana.com/docs/k6/latest/ https://github.com/grafana/k6-operator/tree/main kubectl create -k ../clusters/kind/apps/k6test/ Different tests Smoke test Average Load test Stress test Soak test ##4 We are focusing on the soak test in the following. We use Prometheus to examine the metrics. For this to work, we need to configure Prometheus to accept remote write: https://prometheus.io/docs/prometheus/latest/feature_flags/#remote-write-receiver","title":"4. Soak test with k6s"},{"location":"04-k6s/#k6","text":"K6 could have been named Kapablo . https://grafana.com/docs/k6/latest/ https://github.com/grafana/k6-operator/tree/main kubectl create -k ../clusters/kind/apps/k6test/","title":"K6"},{"location":"04-k6s/#different-tests","text":"Smoke test Average Load test Stress test Soak test ##4 We are focusing on the soak test in the following. We use Prometheus to examine the metrics. For this to work, we need to configure Prometheus to accept remote write: https://prometheus.io/docs/prometheus/latest/feature_flags/#remote-write-receiver","title":"Different tests"},{"location":"05-remote-debug/","text":"Connect a debugger Attaching a debugger to a running application. Prerequisite is that the application is configured with the proper jvm arguments: ##6 -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005 In addition, you need to add \"Remote vm debugging\" to the IntelliJ debug configuration. export POD=$(kubectl get pods -o=jsonpath='{range .items..metadata}{.name}{\"\\n\"}{end}'|grep k8sdebug) echo \"Pod: $POD\" kubectl port-forward $POD 5005:5005 Create a breakpoint the controller (##2), and observe the pause. What happens to the running k6 tests? Pro-tip: Remove probes in order to avoid Kubernetes restart of pod when sleeping on breakpoint.","title":"5. Remote debug"},{"location":"05-remote-debug/#connect-a-debugger","text":"Attaching a debugger to a running application. Prerequisite is that the application is configured with the proper jvm arguments: ##6 -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005 In addition, you need to add \"Remote vm debugging\" to the IntelliJ debug configuration. export POD=$(kubectl get pods -o=jsonpath='{range .items..metadata}{.name}{\"\\n\"}{end}'|grep k8sdebug) echo \"Pod: $POD\" kubectl port-forward $POD 5005:5005 Create a breakpoint the controller (##2), and observe the pause. What happens to the running k6 tests? Pro-tip: Remove probes in order to avoid Kubernetes restart of pod when sleeping on breakpoint.","title":"Connect a debugger"},{"location":"06-jkube/","text":"JKube Insert your laptop into running cluster Eclipse JKube can hijack a service in the cluster on your behalf. You replace the current application in the cluster with your local laptop. Doc: https://eclipse.dev/jkube/docs/kubernetes-maven-plugin/#jkube:remote-dev You need to suspend flux, to avoid reconciliation which would reset the manual configuration. You also need to adjust your Kubernetes context so you are in the correct namespace. Steps: Start k8sdebug locally, change output Check output from currently running cluster: http://k8sdebug-app.local.gd:31090/ Ensure you have a connection to k8s, and that you have the apps namespace active. You should see the k8sdebug pod: kubectl config set-context --current --namespace=apps kubectl get pods Suspend Flux, so it does not reset config: flux suspend kustomization k8sdebug Start k8sdebug locally, change output pom.xml has configuration for hijack. Do: cd ../spring-app ./mvnw k8s:remote-dev Check output again: http://k8sdebug-app.local.gd:31090/ With the steps above, you should observe output from your locally running application. Have a look the pods in the namespace - you have got a new one: kubectl get pods When done, stop maven job with CTRL+C, and remember to resume Flux: flux resume kustomization k8sdebug","title":"6. JKube"},{"location":"06-jkube/#jkube","text":"Insert your laptop into running cluster Eclipse JKube can hijack a service in the cluster on your behalf. You replace the current application in the cluster with your local laptop. Doc: https://eclipse.dev/jkube/docs/kubernetes-maven-plugin/#jkube:remote-dev You need to suspend flux, to avoid reconciliation which would reset the manual configuration. You also need to adjust your Kubernetes context so you are in the correct namespace. Steps: Start k8sdebug locally, change output Check output from currently running cluster: http://k8sdebug-app.local.gd:31090/ Ensure you have a connection to k8s, and that you have the apps namespace active. You should see the k8sdebug pod: kubectl config set-context --current --namespace=apps kubectl get pods Suspend Flux, so it does not reset config: flux suspend kustomization k8sdebug Start k8sdebug locally, change output pom.xml has configuration for hijack. Do: cd ../spring-app ./mvnw k8s:remote-dev Check output again: http://k8sdebug-app.local.gd:31090/ With the steps above, you should observe output from your locally running application. Have a look the pods in the namespace - you have got a new one: kubectl get pods When done, stop maven job with CTRL+C, and remember to resume Flux: flux resume kustomization k8sdebug","title":"JKube"},{"location":"07-kubernetes-debug/","text":"ssh into the pod export POD=$(kubectl get pods -o=jsonpath='{range .items..metadata}{.name}{\"\\n\"}{end}'|grep k8sdebug) kubectl exec -it $POD -- bash What about distroless images? A distroless image contains only the minimal setup for the application. Or the running image lacks tools to use for debugging Attach Kubernetes debugger kubectl debug --help ... # Create a copy of mypod adding a debug container and attach to it kubectl debug mypod -it --image=busybox --copy-to=my-debugger # Create an interactive debugging session on a node and immediately attach to it. # The container will run in the host namespaces and the host's filesystem will be mounted at /host kubectl debug node/mynode -it --image=busybox Copy the failing container, examining it with ubuntu image: export POD=$(kubectl get pods -o=jsonpath='{range .items..metadata}{.name}{\"\\n\"}{end}'|grep k8sdebug) kubectl debug -it $POD --image bellsoft/liberica-openjdk-debian:23-cds apt-get install procps Find the pid of the java process and try to access the root: ls /proc/PID/root Attach an ephemeral container to the original Using the same image as the original container: kubectl debug -it k8sdebug-$HASH --image k8sdebug:manual --target k8sdebug -- bash Find the pid (by looking at /proc ) and examine the directory structure: ls /proc/6/root/workspace/org/springframework/ Jcmd does not work with this, unfortunately. What you can consider making, is an image which is functionally identical, but that contains a shell for debugging. And possibly other tools: k8sdebug-dev:... Distroless images The problem is this: https://dev.to/chainguard/debugging-distroless-images-with-kubectl-and-cdebug-1dm0 Dockerfile: FROM bellsoft/liberica-openjdk-debian:23 USER 1000 docker build -t debug:image . kind load docker-image debug:image --name k8sdebug Using the same type, to easy loading: kubectl debug -it k8sdebug-$HASH --image bellsoft/liberica-openjdk-debian:23 --target k8sdebug Create a copy of the pods with problems, and attach to it: kubectl debug -it k8sdebug-$HASH --image k8sdebug:manual --share-processes --copy-to=debug -- bash The attached debug container does not have probes, so it won't get restarted.","title":"7. Kubernetes debugging"},{"location":"07-kubernetes-debug/#ssh-into-the-pod","text":"export POD=$(kubectl get pods -o=jsonpath='{range .items..metadata}{.name}{\"\\n\"}{end}'|grep k8sdebug) kubectl exec -it $POD -- bash","title":"ssh into the pod"},{"location":"07-kubernetes-debug/#what-about-distroless-images","text":"A distroless image contains only the minimal setup for the application. Or the running image lacks tools to use for debugging","title":"What about distroless images?"},{"location":"07-kubernetes-debug/#attach-kubernetes-debugger","text":"kubectl debug --help ... # Create a copy of mypod adding a debug container and attach to it kubectl debug mypod -it --image=busybox --copy-to=my-debugger # Create an interactive debugging session on a node and immediately attach to it. # The container will run in the host namespaces and the host's filesystem will be mounted at /host kubectl debug node/mynode -it --image=busybox","title":"Attach Kubernetes debugger"},{"location":"07-kubernetes-debug/#copy-the-failing-container-examining-it-with-ubuntu-image","text":"export POD=$(kubectl get pods -o=jsonpath='{range .items..metadata}{.name}{\"\\n\"}{end}'|grep k8sdebug) kubectl debug -it $POD --image bellsoft/liberica-openjdk-debian:23-cds apt-get install procps Find the pid of the java process and try to access the root: ls /proc/PID/root","title":"Copy the failing container, examining it with ubuntu image:"},{"location":"07-kubernetes-debug/#attach-an-ephemeral-container-to-the-original","text":"Using the same image as the original container: kubectl debug -it k8sdebug-$HASH --image k8sdebug:manual --target k8sdebug -- bash Find the pid (by looking at /proc ) and examine the directory structure: ls /proc/6/root/workspace/org/springframework/ Jcmd does not work with this, unfortunately. What you can consider making, is an image which is functionally identical, but that contains a shell for debugging. And possibly other tools: k8sdebug-dev:...","title":"Attach an ephemeral container to the original"},{"location":"07-kubernetes-debug/#distroless-images","text":"The problem is this: https://dev.to/chainguard/debugging-distroless-images-with-kubectl-and-cdebug-1dm0 Dockerfile: FROM bellsoft/liberica-openjdk-debian:23 USER 1000 docker build -t debug:image . kind load docker-image debug:image --name k8sdebug Using the same type, to easy loading: kubectl debug -it k8sdebug-$HASH --image bellsoft/liberica-openjdk-debian:23 --target k8sdebug Create a copy of the pods with problems, and attach to it: kubectl debug -it k8sdebug-$HASH --image k8sdebug:manual --share-processes --copy-to=debug -- bash The attached debug container does not have probes, so it won't get restarted.","title":"Distroless images"},{"location":"08-jcmd-jfr-jmc/","text":"JCMD, JFR and JMC There are several ways of getting a Java Flight Recorder (JFR) recording. You can give command line options, or get a JFR recording started ad hoc. The latter is what I will cover here. JCMD First question is: Do you have JCMD available? If you do not, you want to get it installed somehow. Copy tools out of image If uncertain what is inside the image: docker run --rm -it bellsoft/liberica-openjdk-debian:23-cds bash Here, we just copy a JDK out of an image. An alternative would be to explode a distribution, and use that. This feels somewhat easier, making me more confident that the JDK is compatible. IMAGE=bellsoft/liberica-openjdk-debian:23-cds docker pull --platform linux/amd64 $IMAGE docker create --platform linux/amd64 --name tmp-del $IMAGE docker cp tmp-del:/usr/lib/jvm/jdk-23-bellsoft-x86_64/ jdk Then you copy the full jdk into your container: export POD=$(kubectl get pods -o=jsonpath='{range .items..metadata}{.name}{\"\\n\"}{end}'|grep k8sdebug) kubectl cp jdk $POD:/tmp/jdk kubectl exec -it $POD -- bash Inside the container set the path: java -version export JAVA_HOME=/tmp/jdk export PATH=/tmp/jdk/bin:$PATH java -version jcmd jcmd 7 JFR.start jcmd 7 JFR.stop jcmd 7 JFR.dump name=1 filename=/tmp/filename1.jfr Copy out the image to your local machine: kubectl cp k8sdebug-$HASH:/tmp/filename1.jfr filename1.jfr Will probably work best if the same distro. Jattach To copy a whole JDK feels both wasteful, and dirty. An alternative is to use the jattach tool (if you trust its origins): https://github.com/jattach/jattach/releases export POD=$(kubectl get pods -o=jsonpath='{range .items..metadata}{.name}{\"\\n\"}{end}'|grep k8sdebug) curl -L -o - https://github.com/jattach/jattach/releases/download/v2.2/jattach-linux-x64.tgz|tar -xzf - kubectl cp jattach $POD:/tmp/jattach What if you get: level=error msg=\"exec failed: unable to start container process: exec: \\\"tar\\\": executable file not found in $PATH\" Just pipe it: cat jattach| kubectl exec -i POD-$HASH -- bash -c \"cat > /tmp/jattach\" The commands are then analogous to jcmd from the JDK distribution. kubectl exec -it k8sdebug-app-$HASH -- bash /tmp/jattach 7 jcmd help /tmp/jattach 7 jcmd JFR.start /tmp/jattach 7 jcmd JFR.start /tmp/jattach 7 jcmd JFR.stop /tmp/jattach 7 jcmd JFR.dump name=1 filename=$PWD/filename1.jfr jcmd 7 GC.class_histogram|more JMC Java Mission Control (JMC) is a GUI for JFR: https://github.com/openjdk/jmc Let's get a more representative dump: suspend flux use the image with the \"manual\" tag run the tests kubectl create -k ../clusters/kind/apps/k6test/","title":"8. JCMD / JFR / JMC"},{"location":"08-jcmd-jfr-jmc/#jcmd-jfr-and-jmc","text":"There are several ways of getting a Java Flight Recorder (JFR) recording. You can give command line options, or get a JFR recording started ad hoc. The latter is what I will cover here.","title":"JCMD, JFR and JMC"},{"location":"08-jcmd-jfr-jmc/#jcmd","text":"First question is: Do you have JCMD available? If you do not, you want to get it installed somehow.","title":"JCMD"},{"location":"08-jcmd-jfr-jmc/#copy-tools-out-of-image","text":"If uncertain what is inside the image: docker run --rm -it bellsoft/liberica-openjdk-debian:23-cds bash Here, we just copy a JDK out of an image. An alternative would be to explode a distribution, and use that. This feels somewhat easier, making me more confident that the JDK is compatible. IMAGE=bellsoft/liberica-openjdk-debian:23-cds docker pull --platform linux/amd64 $IMAGE docker create --platform linux/amd64 --name tmp-del $IMAGE docker cp tmp-del:/usr/lib/jvm/jdk-23-bellsoft-x86_64/ jdk Then you copy the full jdk into your container: export POD=$(kubectl get pods -o=jsonpath='{range .items..metadata}{.name}{\"\\n\"}{end}'|grep k8sdebug) kubectl cp jdk $POD:/tmp/jdk kubectl exec -it $POD -- bash Inside the container set the path: java -version export JAVA_HOME=/tmp/jdk export PATH=/tmp/jdk/bin:$PATH java -version jcmd jcmd 7 JFR.start jcmd 7 JFR.stop jcmd 7 JFR.dump name=1 filename=/tmp/filename1.jfr Copy out the image to your local machine: kubectl cp k8sdebug-$HASH:/tmp/filename1.jfr filename1.jfr Will probably work best if the same distro.","title":"Copy tools out of image"},{"location":"08-jcmd-jfr-jmc/#jattach","text":"To copy a whole JDK feels both wasteful, and dirty. An alternative is to use the jattach tool (if you trust its origins): https://github.com/jattach/jattach/releases export POD=$(kubectl get pods -o=jsonpath='{range .items..metadata}{.name}{\"\\n\"}{end}'|grep k8sdebug) curl -L -o - https://github.com/jattach/jattach/releases/download/v2.2/jattach-linux-x64.tgz|tar -xzf - kubectl cp jattach $POD:/tmp/jattach What if you get: level=error msg=\"exec failed: unable to start container process: exec: \\\"tar\\\": executable file not found in $PATH\" Just pipe it: cat jattach| kubectl exec -i POD-$HASH -- bash -c \"cat > /tmp/jattach\" The commands are then analogous to jcmd from the JDK distribution. kubectl exec -it k8sdebug-app-$HASH -- bash /tmp/jattach 7 jcmd help /tmp/jattach 7 jcmd JFR.start /tmp/jattach 7 jcmd JFR.start /tmp/jattach 7 jcmd JFR.stop /tmp/jattach 7 jcmd JFR.dump name=1 filename=$PWD/filename1.jfr jcmd 7 GC.class_histogram|more","title":"Jattach"},{"location":"08-jcmd-jfr-jmc/#jmc","text":"Java Mission Control (JMC) is a GUI for JFR: https://github.com/openjdk/jmc Let's get a more representative dump: suspend flux use the image with the \"manual\" tag run the tests kubectl create -k ../clusters/kind/apps/k6test/","title":"JMC"},{"location":"key-points/","text":"Summary Caveat emptor: The setup in this repository is not a production ready setup, as I have done a lot of shortcuts with regard to security tightening. With a tighter setup, you might find that some of the techniques are no longer applicable. There are many ways you can debug. You now have more options to which method you can use Soak testing is a nifty way of introducing load Metrics rule Good and representative metrics, can be used in a production dashboard too Security profiles may give you grief, as you would need to restart the application to adjust settings Plan ahead for potential trouble Remember to remove temporary access rights when not needed Which method you can use for introspection of a container depends on the Kubernetes flavor, and how the image is build / set up It can be taxing to figure out how to attach a container with the correct setup","title":"Key points"},{"location":"key-points/#summary","text":"Caveat emptor: The setup in this repository is not a production ready setup, as I have done a lot of shortcuts with regard to security tightening. With a tighter setup, you might find that some of the techniques are no longer applicable. There are many ways you can debug. You now have more options to which method you can use Soak testing is a nifty way of introducing load Metrics rule Good and representative metrics, can be used in a production dashboard too Security profiles may give you grief, as you would need to restart the application to adjust settings Plan ahead for potential trouble Remember to remove temporary access rights when not needed Which method you can use for introspection of a container depends on the Kubernetes flavor, and how the image is build / set up It can be taxing to figure out how to attach a container with the correct setup","title":"Summary"}]}